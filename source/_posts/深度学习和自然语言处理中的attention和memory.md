---
title: 深度学习和自然语言处理中的attention
date: 2017-04-17 18:31:24
tags: deep learning
---
http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/
http://blog.csdn.net/malefactor/article/details/50550211
## Encoder-Decoder
要提文本处理领域的AM模型，就不得不先谈Encoder-Decoder框架，因为目前绝大多数文献中出现的AM模型是附着在Encoder-Decoder框架下的，当然，其实AM模型可以看作一种通用的思想，本身并不依赖于Encoder-Decoder模型，这点需要注意。
<!--more-->
![](/img/dl_am_1.png)

Encoder-Decoder框架可以这么直观地去理解：可以把它看作适合处理由一个句子（或篇章）生成另外一个句子（或篇章）的通用处理模型。对于句子对<X,Y>，我们的目标是给定输入句子X，期待通过Encoder-Decoder框架来生成目标句子Y。X和Y可以是同一种语言，也可以是两种不同的语言。而X和Y分别由各自的单词序列构成：
![](/img/dl_am_2.png)
Encoder对输入X进行编码，将输入句子通过非线性变换转化为中间语义表示C：
![](/img/dl_am_3.png)
对于Decoder来说，其任务是根据句子X的中间语义表示C和之前已生成的历史信息y1,y2,...yi-1来生成i时刻要生成的单词i
![](/img/dl_am_4.png)
每个yi都依次生成，那么看来起就是整个系统根据输入句子X生成了目标句子Y

Encoder和Decoder是非常通用的模型，至于encoder,decoder具体使用的常见的有cnn/rnn/BiRNN/GRU/LSTM/Deep LSTM等，这里的变化组合很多。例如可以使用CNN作为Encoder，用RNN作为Decoder；用BiRNN做为Encoder，用深层LSTM作为Decoder，那么就是一个创新。

Encoder-Decoder是个创新游戏大杀器，一方面如上所述，可以搞各种不同的模型组合，另外一方面它的应用场景多得不得了，比如对于机器翻译来说，<X,Y>就是对应不同语言的句子，比如X是英语句子，Y是对应的中文句子翻译。再比如对于文本摘要来说，X就是一篇文章，Y就是对应的摘要；再比如对于对话机器人来说，X就是某人的一句话，Y就是对话机器人的应答.

## Attention Model

在没有引入attention模型中，目标句子Y中每个单词的生成过程如下：
    y1 = f(C)
    y2 = f(C,y1)
    y3 = f(C,y1,y2)
f是decoder的非线性变换函数。在生成目标句子的单词时，不论生成哪个单词，是y1,y2也好，还是y3也好，他们使用的句子X的语义编码C都是一样的，没有任何区别。而语义编码C是由句子X的每个单词经过Encoder 编码产生的，这意味着不论是生成哪个单词，y1,y2还是y3，其实句子X中任意单词对生成某个目标单词yi来说影响力都是相同的，没有任何区别（其实如果Encoder是RNN的话，理论上越是后输入的单词影响越大，并非等权的，估计这也是为何Google提出Sequence to Sequence模型时发现把输入句子逆序输入做翻译效果会更好的小Trick的原因）。
理解AM模型的关键就是这里，即由固定的中间语义表示C换成了根据当前输出单词来调整成加入注意力模型的变化的Ci。增加了AM模型的Encoder-Decoder框架理解起来如图所示。
![](/img/dl_am_5.png)
目标句子Y中每个单词的生成过程如下：
    y1 = f1(C1)
    y2 = f1(C2,y1)
    y3 = f1(C3,y1,y2)
每个Ci可能对应着不同的源语句子单词的注意力分配概率分布.
非AM模型的Encoder-Decoder框架进行细化，Encoder采用RNN模型，Decoder也采用RNN模型，这是比较常见的一种模型配置
![](/img/dl_am_6.png)

![](/img/dl_am_7.png)
对于采用RNN的Decoder来说，如果要生成yi单词，在时刻i，我们是可以知道在生成Yi之前的隐层节点i时刻的输出值Hi的.而我们的目的是要计算生成Yi时的输入句子单词“Tom”、“Chase”、“Jerry”对Yi来说的注意力分配概率分布，那么可以用i时刻的隐层节点状态Hi去一一和输入句子中每个单词对应的RNN隐层节点状态hj进行对比，即通过函数F(hj,Hi)来获得目标单词Yi和每个输入单词对应的对齐可能性，这个F函数在不同论文里可能会采取不同的方法，然后函数F的输出经过Softmax进行归一化就得到了符合概率分布取值区间的注意力分配概率分布数值。图5显示的是当输出单词为“汤姆”时刻对应的输入句子单词的对齐概率。绝大多数AM模型都是采取上述的计算框架来计算注意力分配概率分布信息，区别只是在F的定义上可能有所不同。

一般文献里会把AM模型看作是单词对齐模型，这是非常有道理的。目标句子生成的每个单词对应输入句子单词的概率分布可以理解为输入句子单词和这个目标生成单词的对齐概率，这在机器翻译语境下是非常直观的：传统的统计机器翻译一般在做的过程中会专门有一个短语对齐的步骤，而注意力模型其实起的是相同的作用。在其他应用里面把AM模型理解成输入句子和目标句子单词之间的对齐概率也是很顺畅的想法
