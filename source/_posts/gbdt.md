---
title: gbdt
date: 2017-01-06 17:25:04
tags: 机器学习
---
## Boosting
Boosting是彝族可将弱学习器提升为强学习器的算法.这族算法的工作机制类似:先从初始训练集训练出一个基学习器,在根据学习器的表现对训练样本分布进行调整,使得先前基学习器做错的训练样本在后续受到更多关注,然后基于调整后的样本分布来训练下一个基学习器;如此重复进行,直至基学习器数目达到事先指定的值T,最终将这T个基学习器进行权结合.
## GBDT
GBDT(Gradient Boosting Decision Tree)是一种迭代的决策树算法,该算法由多棵决策树组成,通过所有树进行投票来进行分类.这是一种泛化能力(generalization)比较强的算法.
<!--more-->
GBDT是一个加性回归模型,通过boosting迭代的构造一组弱学习器,相对LR的优势,如不需要做特征的归一化,自动进行特征选择,模型可解释性比较好,可以适应多种损失函数如,Squareloss,LogLoss等.每一次建立模型是在之前建立模型的损失函数的梯度下降方向
GBDT的核心在于,每一棵树学的是之前所有树结论和的残差,残差为与预测值的差值.

而Gradient Boost与传统的Boost的区别是，每一次的计算是为了减少上一次的残差(residual)，而为了消除残差，我们可以在残差减少的梯度(Gradient)方向上建立一个新的模型。所以说，在Gradient Boost中，每个新的模型的简历是为了使得之前模型的残差往梯度方向减少，与传统Boost对正确、错误的样本进行加权有着很大的区别。


## 随机森林(Random Forest)

随机森林是Bagging的一个扩展变体.RF在以决策树为基学习器构建Bagging集成 的基础上,进一步在决策树的训练过程中引入了随机属性选择.传统决策树在选择划分属性时是在当前结点的属性集合(假定有d个属性)中选择一个最优属性;而在随机森林中,对基决策树的每个结点,先从该结点的属性集合中随机选择一个包含k个属性的子集,然后再从这个子集中选择一个最优属性用户划分.一个情况下,k=log2d

行采样：使用bagging中的bootstrap aggregating方法，采用有放回的方式，假设输入样本为N个，那么采样的样本次数也是N。
列采样：从M个feature中，选择m个(m << M)。之后就是对采样之后的数据使用完全分裂的方式建立出决策树。

  随机选取训练样本集
  随机选取分裂属性集
  每棵树任其生长，不进行剪枝

随机森林是一个最近比较火的算法，它有很多的优点：
  在数据集上表现良好
  在当前的很多数据集上，相对其他算法有着很大的优势
  它能够处理很高维度（feature很多）的数据，并且不用做特征选择
  在训练完后，它能够给出哪些feature比较重要
  在创建随机森林的时候，对generlization error使用的是无偏估计
  训练速度快
  在训练过程中，能够检测到feature间的互相影响
  容易做成并行化方法
  实现比较简单
### 回归或分类
器学习算法经常被用于预测分析。在商业中，预测分析可以用于告诉企业未来最有可能发生什么。例如，使用预测分析算法，在线 T 恤零售商可以使用当前的数据来预测下个月他们将会售出多少 T 恤。
预测问题分为两大类：

  - 回归问题（Regression Problems）：我们想要预测的变量是数字（例如，房子的价格）
  - 分类问题（Classification Problems）：我们想要预测的变量是「是/否」的答案（例如，某一设备是否经历设备故障）
#### 线性回归

  线性回归，也称为「最小二乘回归（least squares regression）」，是线性模型的最标准的形式。对于回归问题（我们设法预测的变量是数字），线性回归是最简单的线性模型。
#### 逻辑回归

  逻辑回归是为分类问题进行简单调整过的线性回归（我们设法预测的变量是「是/否」的答案）。由于其构造，逻辑回归非常适合于分类问题
#### 线性回归和逻辑回归的缺点
  线性回归和逻辑回归都有着相同的缺点。两者都具有「过拟合（overfit）」的趋势，这意味着模型太适应于数据而牺牲了推广到先前未知的数据的能力。因此，这两个模型经常需要进行规范，这意味着它们有一定的惩罚（penalty）以防止过拟合。另一个线性模型的缺点是，因为它们太简单了，所以往往不能预测更复杂的行为。
### 树型模型
树型模型有助于探索数据集，并可视化预测的决策规则。当你听到关于树型模型的东西时，你可以将其想成是决策树或分支操作序列。树型模型高度精确、稳定且更易于解释。与线性模型相反，它们可以映射非线性关系以求解问题。
#### 决策树（decision tree）

决策树是一种使用分支方法（branching method）来显示决策的每个可能结果的图。例如，如果你想要订购莴苣、浇头和沙拉酱，决策树可以绘制出所有可能的结果（或者你可能最终得到的沙拉的品种）。

为了创建或者训练决策树，我们采用我们过去训练模型的数据，并找出哪些属性可以最佳分割目标训练集。

例如，我们在信用卡欺诈中使用决策树。我们可以发现最佳的欺诈风险预测的属性是消费明细（例如，有信用卡用户有非常大的消费）。这可能是第一次分割（或分支）——那些有着异常高消费的卡和没有的卡。然后我们使用第二个最佳属性（例如，经常使用的信用卡）来创建下一次分割。然后我们可以继续直到我们有足够的属性来满足我们的需要。

#### 随机森林（random forest）

随机森林是许多决策树的平均，每个决策树都用数据的随机样本训练。森林中的每个独立的树都比一个完整的决策树弱，但是通过将它们结合，我们可以通过多样性获得更高的整体表现。

随机森林是当今机器学习中非常流行的算法。它非常容易训练（或构建），且它往往表现良好。它的缺点是，相比于其他算法，其输出预测可能较慢。所以当你需要快如闪电般地预测，你也许不会使用它。

#### 梯度提升（gradient boosting）

梯度提升和随机森林类似，都是由「弱」决策树构成的。最大的区别是，在梯度提升中树是被一个接一个相继训练的。每个随后的树主要用被先前树错误识别的数据进行训练。这使得梯度提升更少地集中在容易预测的情况并更多地集中在困难的情况。

梯度提升训练速度也很快且表现非常好。然而，训练数据的小变化可以在模型中产生彻底的改变，因此它可能不会产生最可解释的结果。

### 神经网络

生物学中的神经网络是互相交换信息的相互连接的神经元。这个想法现在已经适用于机器学习的世界，并被称为人工神经网络（ANN）。深度学习（deep learning）是一个经常出现的词，是指几层连续放置的人工神经网络。

人工神经网络（ANN）包含了许多可以学习类似人脑的认知能力的模型。其它算法不能处理的极其复杂的任务（如图像识别），神经网络就可以办到。然而，就像人类的大脑，它需要很长时间来训练模型，且需要很多的能量


### GBDT和xgboost

传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。Shrinkage（缩减），相当于学习速率（xgboost中的eta）。xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（补充：传统GBDT的实现也有学习速率）列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。对缺失值的处理。对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向。xgboost工具支持并行。boosting不是一种串行的结构吗?怎么并行的？注意xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。可并行的近似直方图算法。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。
