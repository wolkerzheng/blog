---
title: 向量空间模型
date: 2017-01-05 20:59:57
tags: 机器学习
---
### vsm
VSM(Vector Space Model):
把文本内容的处理简化为向量空间中的向量运算,并且它以空间上的相似度表达语义的相似度,直观易懂.当文档被表示为文档空间的向量,就可以通过计算向量之间的相似度来度量文档间的相似性.文本处理中最常用的相似度量方法是余弦距离<!--more-->
M个无序特征项ti，词根/词/短语/其他每个文档dj可以用特征项向量来表示（a1j,a2j，…，aMj）权重计算，N个训练文档AM*N= (aij) 文档相似度比较1）Cosine计算，余弦计算的好处是，正好是一个介于0到1的数，如果向量一致就是1，09如果正交就是0，符合相似度百分比的特性,余弦的计算方法为，向量内积/各个向量的模的乘积.2）内积计算，直接计算内积，计算强度低，但是误差大。
向量空间模型 （或词组向量模型) 是一个应用于信息过滤，信息撷取，索引 以及评估相关性的代数模型。SMART是首个使用这个模型的信息检索系统。
文件（语料）被视为索引词（关键词）形成的多次元向量空间， 索引词的集合通常为文件中至少出现过一次的词组。
搜寻时，输入的检索词也被转换成类似于文件的向量，这个模型假设，文件和搜寻词的相关程度，可以经由比较每个文件(向量）和检索词（向量）的夹角偏差程度而得知。
实际上，计算夹角向量之间的余弦比直接计算夹角容易：
余弦为零表示检索词向量垂直于文件向量，即没有符合，也就是说该文件不含此检索词。
通过上述的向量空间模型，文本数据就转换成了计算机可以处理的结构化数据，两个文档之间的相似性问题转变成了两个向量之间的相似性问题。

向量空间模型的关键在于特征向量的选取和特征向量的权值计算两个部分.

1．文档向量的构造
　　对于任一文档d_j ∈ D，我们可以把它表示为如下t维向量的形式：

\overline{d}_j =(w_{1j},w_{2j},\cdots,w_{tj})
  向量分量wtj代表第i个标引词ki在文档dj中所具有的权重，t为系统中标引词的总数。在布尔模型中，wtj的取值范围是{0，1}；在向量空间模型中，由于采用“部分匹配”策略，wtj的取值范围是一个连续的实数区间[0，1]。

### TF-IDF
TF-IDF（term frequency–inverse document frequency）是一种用于信息检索与数据挖掘的常用加权技术。
    词频(TF) = 某个词在文章中的出现次数
标准化:
    词频(TF) = 某个词在文章中的出现次数/文章的总词数
计算逆文档频率:
    逆文档频率(IDF) = log(语料库的文档总数/包含该词的文档数+1)
如果一个词越常见，那么分母就越大，逆文档频率就越小越接近0。分母加1，是为了避免分母为0（即所有文档都不包含该词）。log表示对得到的值取对数。
    TF-IDF = 词频(TF) * 逆文档频率(IDF)
所以,TF-IDF与一个词在文档中的出现次数成正比,与该词在整个语言中的出现次数成反比.
TF-IDF权重计算方法经常会和余弦相似度(cosine similarity)一同使用於向量空间模型中，用以判断两份文件之间的相似性。
找出相似文章:
    （1）使用TF-IDF算法，找出两篇文章的关键词；
　　（2）每篇文章各取出若干个关键词（比如20个），合并成一个集合，计算每篇文章对于这个集合中的词的词频（为了避免文章长度的差异，可以使用相对词频）；
　　（3）生成两篇文章各自的词频向量；
　　（4）计算两个向量的余弦相似度，值越大就表示越相似。
### 词袋模型(BoW)
最初被用在文本分类中，将文档表示成特征矢量。
该模型忽略掉文本的语法和语序等要素，将其仅仅看作是若干个词汇的集合，文档中每个单词的出现都是独立的。BoW使用一组无序的单词(words)来表达一段文字或一个文档.
基本思想：假定对于一个文本，忽略其语序和语法，句法，仅仅将其看做是一些词汇的集合，而文本中的每个词汇都是独立的。简单说就是将每篇文档看作成一个袋子（因为里面装的都是词汇，所以称为词袋），然后看这个袋子装的都是些什么词汇，将其分类。如果文档中猪、马、牛、羊、山谷、土地、拖拉机这样的词汇多些，而银行、大厦、汽车、公园这样的词汇少些，我们就倾向于判断它是一篇描绘乡村的文档，而不是描述城镇的。举个例子，有如下两个文档：
文档一：Bob likes to play basketball, Jim likes too.
文档二：Bob also likes to play football games.

基于这两个文本文档，构造一个词典：
Dictionary = {1:”Bob”, 2. “like”, 3. “to”, 4. “play”, 5. “basketball”, 6. “also”, 7. “football”，8. “games”, 9. “Jim”, 10. “too”}。

这个词典一共包含10个不同的单词，利用词典的索引号，上面两个文档每一个都可以用一个10维向量表示（用整数数字0~n（n为正整数）表示某个单词在文档中出现的次数）：
1：[1, 2, 1, 1, 1, 0, 0, 0, 1, 1]
2：[1, 1, 1, 1 ,0, 1, 1, 1, 0, 0]

向量中每个元素表示词典中相关元素在文档中出现的次数(下文中，将用单词的直方图表示)。不过，在构造文档向量的过程中可以看到，我们并没有表达单词在原来句子中出现的次序（这是本Bag-of-words模型的缺点之一，不过瑕不掩瑜甚至在此处无关紧要）。
