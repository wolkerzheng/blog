#decision tree

###决策树的工作原理
在决策树中，每个叶节点都赋予一个类标号。非终结点(non-terminal node)（包括根结点和内部结点）包含属性测试条件，以分开具有不同特性的记录。
一旦构造了决策树，对检验记录进行分类就相当容易了，从数的根节点开始，将测试条件用与检验记录，根据检测结果选择适当的分支。沿着该分支或者到达另一个内部结点，使用新的测试条件，或者到达一个叶节点。到达叶节点后，叶节点的类称号就被赋值给该检验记录

###如何建立决策树
原则上讲，对于给定的数据集，可以构造的决策树的数目达指数级。为了能够在合理的时间内构造出具有一定准确率率的次最优决策树，开发了很多有效算法，这些算法一般都采用贪心策略，在选择划分数据的属性时，采取一系列局部最优决策来构造决策数，Hunt算法就是这种算法。由此衍生了一些：ID3,C4.5和CART
*1. Hunt算法*
在Hunt算法中，通过训练记录相继划分成较纯的子集，以递归方式建立决策树。设Dt是与结点t相关联的训练记录集，而y={y1,y2,...,y3}是类标号，hunt算法递归定义为：
	（1）如果Dt中所有记录都属于同一个类yt，则t是叶结点，用yt标记
	（2）如果Dt中包含属于多个类的记录，则选择一个属性测试条件（attribute test condition），将记录划分成较小的子集。对于测试条件的每个输出，创建一个子女结点，并根据测试结果将Dt中的记录分布到子女结点中。然后，对与每个子女结点，递归调用该算法。
如果属性值的每种组合都在训练数据中出现，且每种组合具有唯一的类标号，则Hunt是有效的。但这种条件过于苛刻，需要附加条件来处理：
	（1）算法的第二步所创建的子女结点可能为空，即不存在与这些结点相关联的记录。如果没有一个训练记录包含与这样的结点相关联的属性值组合，这种情形可能发生。这时，该结点成为叶结点，类标号为其父节点上训练记录中的多数类
	（2）在第二步中，如果与Dt相关联的所有记录都具有相同的属性值（目标属性除外），则不可能进一步划分这些记录。在这种情况下，该结点为叶节点，其标号为与该结点相关联的训练记录中的多数类。

*2. 决策树的归纳的设计问题*
 	（1）*如何分裂训练记录？*   树增长过程的每个递归步都必须选择一个属性测试条件，将记录划分成较小的子集。为了实现这个，算法必须提供为不同类型的属性指定测试条件的方法，并且提供评估每种测试条件的客观度量
	（2）*如何停止分裂过程？*   需要有结束条件，以终止决策树的生长过程。一个可能的决策是分裂结点，直到所有的记录都属于同一个类，或者所有的记录都具有相同的属性值。尽管两个结点结束条件对于结束决策树归纳算法都是充分的，但是可以使用其他的标准提前终止树的生长过程。

###表示属性测试条件的方法
决策数归纳算法必须为不同类型的属性提供表示属性测试条件和对应输出的方法
*二元属性*     二元属性的测试条件产生两个可能的输出
*标称属性*     由于标称属性有多个属性值，它的测试条件可以用两种方法表示。
*序数属性*     序数属性也可产生二元或多路划分，只要不违背序数属性值的有序性，就可对属性值进行分组。
*连续属性*     对于连续属性来说，测试条件可以是二元输出的比较测试（A<v）或（A>=v）,也可以是具有形如Vi<=A<Vi+1(i=1,...,k)输出的范围查询。

###选择最佳划分的度量
	
