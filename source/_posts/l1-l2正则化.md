---
title: 'l1,l2正则化'
date: 2017-04-13 09:52:30
tags: 机器学习
---
过拟合与规则化
规则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项(regularizer)或惩罚项(penalty term).正则化的作用是让网络偏好学习更小的权值
W* = argmin sum{L(yi,f(xi;w))}+ λ*Ω(w)
L(yi,f(xi;w))损失函数衡量模型（分类或者回归）对第i个样本的预测值f(xi;w)和真实的标签yi之前的误差，第二项是对参数w的规则化函数Ω(w)去约束模型尽量的简单。当λ较小时，偏好最小化原本的代价函数，而λ较大时我们偏好更小的权值。
<!--more-->
L0正则化的值是模型参数中非零参数的个数。
  稀疏的参数可以防止过拟合，因此用L0范数（非零参数的个数）来做正则化项是可以防止过拟合的。L0范数来规则化一个参数矩阵W的话，就是希望W的大部分元素都是0，让参数W是稀疏的。
稀疏规则算子。
L1正则化（Lasso regularization）表示各个参数绝对值之和，sum|Wi|。也叫稀疏规则算子。
  任何的规则化算子，如果他在Wi=0的地方不可微，并且可以分解为一个“求和”的形式，那么这个规则化算子就可以实现稀疏。这说是这么说，W的L1范数是绝对值，|w|在w=0处是不可微，但这还是不够直观。稀疏规则化算子的引入就是为了完成特征自动选择，它会学习地去掉这些没有信息的特征，也就是把这些特征对应的权重置为0。

  既然L0可以实现稀疏，为什么不用L0，而要用L1呢？L0范数很难优化求解（NP难问题），二是L1范数是L0范数的最优凸近似，而且它比L0范数要容易优化求解。

L2正则化（Ridge Regression）表示各个参数的平方的和的开方值 sqrt(sum|Wi**2|)
  L2范数可以防止过拟合，提升模型的泛化能力。



L1,L2区别：
（核心：L2对大数，对outlier离群点更敏感！）
L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。Lasso在特征选择时候非常有用，而Ridge就只是一种规则化而已。
下降速度：最小化权值参数L1比L2变化的快
![](/img/l1l2_2.png)
模型空间的限制：L1会产生稀疏 L2不会。

L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。
![](/img/l1l2_1.png)
