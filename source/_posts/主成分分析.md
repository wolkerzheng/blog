---
title: 主成分分析
date: 2017-03-23 22:04:47
tags: 机器学习
---
## 主成分分析（PCA）
### 简介

http://ufldl.stanford.edu/wiki/index.php/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90
<!--more-->
主成分分析（PCA）是一种能够极大提升无监督特征学习速度的数据降维算法。对于实现白化算法也有很大帮助，很多算法都先用白化算法做预处理步骤。
假设使用图像来训练算法，因为图像中相邻的像素高度相关，输入数据是有一定冗余的。假如正在训练16*16的灰度值图像，记为256维向量X，其中特征值X_j对应每个像素的亮度值。由于相邻像素间的相关性，PCA算法可以将输入向量转换为一个维数低很多的近似向量，且误差也很小。

### 实例和数学背景
在我们的实例中，使用的输入数据集表示为 \textstyle \{x^{(1)}, x^{(2)}, \ldots, x^{(m)}\} ，维度 \textstyle n=2 即 \textstyle x^{(i)} \in \Re^2 。假设我们想把数据从2维降到1维。（在实际应用中，我们也许需要把数据从256维降到50维；在这里使用低维数据，主要是为了更好地可视化算法的行为）。下图是我们的数据集：
![](/img/pca_1.png)
这些数据已经进行了预处理，使得每个特征 \textstyle x_1 和 \textstyle x_2 具有相同的均值（零）和方差。
为方便展示，根据 \textstyle x_1 值的大小，我们将每个点分别涂上了三种颜色之一，但该颜色并不用于算法而仅用于图解。
PCA算法将寻找一个低维空间来投影我们的数据。从下图中可以看出， \textstyle u_1 是数据变化的主方向，而 \textstyle u_2 是次方向。
![](/img/pca_2.png)

也就是说，数据在 \textstyle u_1 方向上的变化要比在 \textstyle u_2 方向上大。为更形式化地找出方向 \textstyle u_1 和 \textstyle u_2 ，我们首先计算出矩阵 \textstyle \Sigma ，如下所示：
![](/img/pca_3.png)
假设 \textstyle x 的均值为零，那么 \textstyle \Sigma 就是x的协方差矩阵。（符号 \textstyle \Sigma ，读"Sigma"，是协方差矩阵的标准符号。虽然看起来与求和符号 \sum_{i=1}^n i 比较像，但它们其实是两个不同的概念。）
数据变化的主方向 \textstyle u_1 就是协方差矩阵 \textstyle \Sigma 的主特征向量，而 \textstyle u_2 是次特征向量。

### 旋转数据
至此，我们可以把 \textstyle x 用 \textstyle (u_1, u_2) 基表达为：
\begin{align}
x_{\rm rot} = U^Tx = \begin{bmatrix} u_1^Tx \\ u_2^Tx \end{bmatrix}
\end{align}
（下标“rot”来源于单词“rotation”，意指这是原数据经过旋转（也可以说成映射）后得到的结果）
对数据集中的每个样本 \textstyle i 分别进行旋转： \textstyle x_{\rm rot}^{(i)} = U^Tx^{(i)} for every \textstyle i ，然后把变换后的数据 \textstyle x_{\rm rot} 显示在坐标图上，可得：
![](/img/pca_4.png)
这就是把训练数据集旋转到 \textstyle u_1，\textstyle u_2 基后的结果。一般而言，运算 \textstyle U^Tx 表示旋转到基 \textstyle u_1,\textstyle u_2, ...,\textstyle u_n 之上的训练数据。矩阵 \textstyle U 有正交性，即满足  \textstyle U^TU = UU^T = I ，所以若想将旋转后的向量 \textstyle x_{\rm rot} 还原为原始数据 \textstyle x ，将其左乘矩阵\textstyle U即可： \textstyle x=U x_{\rm rot} , 验算一下： \textstyle U x_{\rm rot} =  UU^T x = x.
### 数据降维
。。。
