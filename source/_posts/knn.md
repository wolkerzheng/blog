---
title: knn
date: 2016-12-17 20:16:01
tags: 机器学习
---
### 特征选择
好的特征选择能够提升模型性能

特征选择：
1.减少特征数量、降维，使模型泛化能力更强，减少过拟合
2.增强对特征和特征值之间的理解
<!--more-->
* 去掉取值变化小的特征 Removing features with low variance
* 单变量特征选择Univariate feature selection
	单变量特征选择能够对每一个特征进行测试，衡量该特征和响应变量之间的关系，根据得分舍去不好的特征。对于回归和分类问题可以采用卡方检验对特征进行测试。
	*  Pearson相关系数Pearson Correlation
	皮尔森相关系数是一种最简单的，能帮助理解特征和响应变量之间关系的方法，该方法衡量的是变量之间的线性相关性，结果的取值区间为[-1，1]，-1表示完全的负相关(这个变量下降，那个就会上升)，+1表示完全的正相关，0表示没有线性相关。Pearson相关系数的一个明显缺陷是，作为特征排序机制，他只对线性关系敏感。如果关系是非线性的，即便两个变量具有一一对应的关系，Pearson相关性也可能会接近0。
	*  互信息和最大信息系数 Mutual information and maximal information coefficient
	*  距离相关系数（Distance correlation）
	*  基于学习模型的特征排序(Model based ranking)
	这种方法的思路是直接使用你要用的机器学习算法，针对每个单独的特征和响应变量建立预测模型。其实Pearson相关系数等价于线性回归里的标准化回归系数。假如某个特征和响应变量之间的关系是非线性的，可以用基于树的方法（决策树、随机森林）、或者扩展的线性模型等。基于树的方法比较易于使用，因为他们对非线性关系的建模比较好，并且不需要太多的调试。但要注意过拟合问题，因此树的深度最好不要太大，再就是运用交叉验证。
* 线性模型与正则化
###　k-近邻分类器

给一个训练数据集和一个新的实例，在训练数据集中找出与这个新实例最近的k个训练实例，然后统计最近的k个训练实例中所属类别计数最多的那个类，就是新实例的类

三要素:
1.k值的选择
2.距离的度量(欧式距离,马氏距离等)
3.分类决策规则(多数表决规则)

k值的选择:
1.k值越小表明模型越复杂,更加容易过拟合
2.但是k值过大,模型越简单,若k=n,意味着什么点都是训练集中类别最多的那个类

一般k会取一个较小的值,然后使用过交叉验证来确定


k-近邻分类算法
	1．设置参数k,输入待识别样本x;
	2. 计算x与每个训练样本的距离
	3. 选取距离最小的前k个样本，统计其中包含各个类别的样本数k
	4.　class ←　argmax Ki

(1)计算已知类别数据集中的点与当前点之间的距离
(2)按照距离递增次序排序
(3)选取与当前点距离最近的k个点
(4)确定k个点所在类别的出现频率
(5)返回前k个点出现频率最高的类别作为当前点的预测分类

### HMM
