---
title: 句子相似度计算
date: 2017-04-30 10:45:42
tags: 自然语言处理
---
相似度计算方法：
##  Cosine,Jaccard和Shingling
 1.分词
 2.去停用词
 3.stem
<!--more-->
### 余弦相似度
将文档表示为向量空间模型，常用方法：
（1）使用TF-IDF算法，找出两篇文章的关键词；
（2）每篇文章各取出若干个关键词（比如20个），合并成一个集合，计算每篇文章对于这个集合中的词的词频（为了避免文章长度的差异，可以使用相对词频）；
（3）生成两篇文章各自的词频向量；
（4）计算两个向量的余弦相似度，值越大就表示越相似。

假定文档A和B是两个n维向量，A是 [A1, A2, ..., An] ，B是 [B1, B2, ..., Bn] ，则A与B的夹角θ的余弦等于：
![](/img/句子相似度_1.png)
余弦值越接近1，就表明夹角越接近0度，也就是两个向量越相似，这就叫"余弦相似性"。
### Jaccard相似度计算
假设集合A = {0, 1, 2, 5, 6},B = {0, 2, 3, 5, 7, 9}
![](/img/句子相似度_3.png)
增加聚类方法，例如：令
C1 = {0, 1, 2}
C2 = {3, 4}
C3 = {5, 6}
C4 = {7, 8, 9}
例如，C1表示动作类电影集合，C2表示喜剧类电影，C3表示记录片，C4表示恐怖片，然后集合A表示 A_clu = {C1, C3}, B_clu = {C1, C2, C3, C4}，则A和B的Jaccard的相似度：
![](/img/句子相似度_4.png)

### K-Shingling
 A k-shingle is a consecutive(连续) set of k words. So the set of all 1-shingles is exactly the bag of
words model. An alternative name to k-shingle is an k-gram.
例如：
D1 : I am Sam.
D2 : Sam I am.
D3 : I do not like green eggs and ham.
D4 : I do not like them, Sam I am.
The (k = 1)-shingles of D1∪D2∪D3∪D4 are: {[I], [am], [Sam], [do], [not], [like],[green], [eggs], [and], [ham], [them]}.
The (k = 2)-shingles of D1∪D2∪D3∪D4 are: {[I am], [am Sam], [Sam Sam], [Sam I],[am I], [I do], [do not], [not like], [like green], [green eggs], [eggs and],[and ham], [like them], [them Sam]}.
相关资料：
https://www.cs.utah.edu/~jeffp/teaching/cs5955/L4-Jaccard+Shingle.pdf

## 基于Wordnet的语义分析
此部分内容待补充。
Wordnet是一个覆盖范围宽广的英语词汇语义网.Wordnet是一个词典。Wordnet是一个具有等级层次结构的知识库，每一个节点关联某一个概念。
wordnet的计算方法有很多种，有基于路径距离的方法，也有基于信息内容的方法。
### 基于路径距离的方法
基于路径距离的方法，认为如果两个概念在等级层次结构中的距离越近，其相关度越高。最简单的就是通过边的长度来决定概念间的相关度，语义距离定义为两个概念在层次结构上最短路径所包含的节点个数。
- 概念c1和c2的最短路径上的节点个数
- 概念c1和c2的最近公共祖先节点
### 基于信息内容的方法
基于信息内容的方法认为两个概念之间的相关度与二者的信息内容是成正比
的。将P(c)定义为从语料库中随机选择一个词语是概念c的一个实例的概率,正常情况下,每一个词语都可能关联到WordNet的某一个概念上。给定一个概念 c,某一个词语属于该概念的一个实例的概率是P(c),不属于该概念实例的概率是1-P(c) 。由于所有的词语都是根节点的实例,所以P(root)=1 ,某一个节点在层次结构中所处的层级越低,那么它的概率就越低。
相关内容文档：
http://staffwww.dcs.shef.ac.uk/people/S.Fernando/pubs/clukPaper.pdf
## 传统的机器学习模型
例如可以使用训练数据来对逻辑回归模型，随机森林模型等来进行训练。

## 词向量
Word2Vec and CBOW, and now Doc2Vec, Paragraph2Vec, skip-thought vectors.
使用深度学习模型可以计算相似
1.CNN over Word Embeddings：
https://aclweb.org/anthology/K15-1013
2.Skip-thought Vectors
https://github.com/ryankiros/skip-thoughts

##  Explicit Semantic Analysis（ESA）
在整个维基百科（词袋模型）中训练模型，然后将问题转换为一组概念（具有概率）。 概念概率也高，两个问题更相似。
http://www.gabrilovich.com/resources/code/esa/esa.html
## (TensorFlow)面向文本相似度检测的Deep LSTM siamese网络
https://github.com/dhwajraj/deep-siamese-text-similarity
![](/img/句子相似度_2.png)


## 相关文档
https://www.aclweb.org/aclwiki/index.php?title=Paraphrase_Identification_(State_of_the_art)
https://www.kaggle.com/c/quora-question-pairs/discussion/30340
